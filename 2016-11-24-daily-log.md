**볼 책**  <br/>

1. [모으지 않는 연습] (http://www.aladin.co.kr/shop/wproduct.aspx?ItemId=96387260) <br/>
2. [프로세서를 지탱하는...](http://www.aladin.co.kr/shop/wproduct.aspx?ItemId=13772806)
3. 스칼라 프로그래밍:  우리말로된것 우리말로 된것 @교보


**미팅노트**<br/>

1. prepare-lang 수정해서 topology, extra question 달리 해주게 된다.

| category | states| phones |   
|----------|------|------|
|non-stop |
| stop |
| sil |
| vowel |


2. Dan Ellis 따라해보기 (확률 플랏.)
3. ONline decoding 을 꼭지로 넣는것도 괜찮겠다.
4. 낭독체+서울코퍼스 합치기 <br/>
5. articulatory modelling

>낭독체 단어 너무 적다 (930문장, 8천여 단어뿐이다) <br/> 서로 다른 데이터를 합쳐서 해야 할것같다. <br/> 그런데 phone set 통합이 안 되어있다. <br/>우리가 갖고 있는 g2p기준으로 통합하기. <br/>

6. G2P의 최적화 신경쓰기

7. EPD: end point detection 다 했다고 생각하고 칼디가 돌아가요.

8. 쑤도 형태소로 나눠서 word2vec 진행하기

9. CNN: `convolution & pooling의 반복`으로 피쳐 extraction을 한다음에 마지막에만 히든 레이어 아웃풋 해준다음에 기냐 아니냐 답내면 된다.
    `로컬리티의 파괴`
   
10. http://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/ 
11. 하둡 vs 스파크

	하드 디스크 용량은 커졌지만 여기 엑세스 하는 시간은 별로 빨라지지 않았다.<br/>
	그런데 100개의 드라이브가 있고, 각 드라이브는 데이터의 100분의 1을 저장했다고 가정 해 보라. 병렬로 동작한다면 우리는 2분 내에 데이터를 	읽을 수 있다.다시 말해, 하둡은 HDFS(Hadoop Distributed File System)라는 데이터 저장소와 맵리듀스(MapReduce)라는 분석 시스템을 통해 분산 프로그래밍을 수행하는 프레임 워크 인 것이다! 작업을 블록으로 나눠서 클러스터링된 컴터들에 할당한다.
	
	기존에는 이런 것을 RDBMS는라는 친구를 써서 했는데, 요즘같은 시대의 대용량 데이터를 감당하기엔 하둡이 싸다. **한 예로, 2008년 뉴욕타임즈의 130년 분량의 신문기사 1100만 페이지를 하둡을 이용해 하루 만에 200만원의 비용을 들여 PDF로 변환 하였다. 이는 일반서버로 진했을 경우, 약 14년이 소요되는 작업이었다.** <br/> 한마디로 하둡 없이 빅데이터 사용은 불가능했다.



